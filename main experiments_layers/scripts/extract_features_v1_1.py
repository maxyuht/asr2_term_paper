import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from transformers import HubertModel, Wav2Vec2FeatureExtractor
from tqdm import tqdm

# âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
processor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/hubert-base-ls960")
hubert = HubertModel.from_pretrained("facebook/hubert-base-ls960").eval().to("cuda" if torch.cuda.is_available() else "cpu")

# âœ… æ ‡ç­¾æ˜ å°„
label_map = {"happy": 0, "sad": 1, "surprised": 2, "calm": 3}
label_names = {v: k for k, v in label_map.items()}

# âœ… è·¯å¾„è®¾ç½®
data_dir = "./data_v1"
save_dir = "./features_v1_1"  # ğŸš¨ å»ºè®®å¦å­˜ï¼Œé¿å…è¦†ç›–
os.makedirs(save_dir, exist_ok=True)

# âœ… ç”¨äºå¯è§†åŒ–æ”¶é›†å…¨éƒ¨ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆä»ä½¿ç”¨ mean è¿›è¡Œå¯è§†åŒ–ï¼‰
all_features = []
all_labels = []

print("HuBERT is extracting time-sequence features for attention pooling...\n")
for emotion in os.listdir(data_dir):
    emo_path = os.path.join(data_dir, emotion)
    if not os.path.isdir(emo_path):
        continue

    print(f"Working on: {emotion}")
    file_list = [f for f in os.listdir(emo_path) if f.endswith(".wav")]

    for wav in tqdm(file_list, desc=f"{emotion}", ncols=80):
        wav_path = os.path.join(emo_path, wav)
        waveform, _ = librosa.load(wav_path, sr=16000)

        # âœ… æå–æ—¶é—´åºåˆ—ç‰¹å¾
        inputs = processor(waveform, sampling_rate=16000, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = hubert(inputs.input_values.to(hubert.device))
            hidden_states = outputs.last_hidden_state  # shape: [1, T, 768]

        seq_feature = hidden_states.squeeze(0).cpu()  # shape: [T, 768]

        # âœ… ä¿å­˜æ•´ä¸ªåºåˆ—ç‰¹å¾
        save_path = os.path.join(save_dir, wav.replace(".wav", ".pt"))
        torch.save({"feature": seq_feature, "label": label_map[emotion]}, save_path)

        # âœ… å¯è§†åŒ–ä»ä½¿ç”¨ mean å‘é‡
        mean_feature = seq_feature.mean(dim=0).numpy()
        all_features.append(mean_feature)
        all_labels.append(label_map[emotion])

    print(f"Finished {emotion}, processed {len(file_list)} audios.\n")

print("âœ… All sequence features have been saved to ./features_v1/\n")

# âœ… å¯è§†åŒ–ï¼ˆmean åªæ˜¯ç”¨äº t-SNEï¼‰
print("Feature visualization (t-SNE)...")
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
reduced = tsne.fit_transform(np.array(all_features))

plt.figure(figsize=(8, 6))
colors = ['red', 'blue', 'green', 'orange']
for i, label_name in label_names.items():
    idx = [j for j, l in enumerate(all_labels) if l == i]
    plt.scatter(reduced[idx, 0], reduced[idx, 1], label=label_name, alpha=0.7, s=30, color=colors[i])

plt.title("t-SNE Visualization of HuBERT Features (mean for vis)")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("tsne_visualization_v1.png")
plt.show()

print("ğŸ“Š t-SNE saved as tsne_visualization_v1.png")
